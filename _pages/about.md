---
layout: about
title: About
permalink: /
subtitle: Ph.D. Student, School of Computing & Augmented Intelligence, Arizona State University.


profile:
  align: right
  image: IMG_1048.JPG
  image_circular: false # crops the image to make it circular
  more_info: >
   
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page

announcements:
  enabled: true # includes a list of news items
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit: 5 # leave blank to include all the news in the `_news` folder

---

I am a Ph.D. student at Arizona State University, working under the supervision of Professor Yezhou Yang.

My research centers on evaluating the multimodal abstract reasoning capabilities of vision-language models (VLMs) and large language models (LLMs), with a particular emphasis on spatial visualization and visual analogy problems. These cognitively demanding tasks reflect core aspects of human intelligence, and my goal is to advance our understanding of human cognition by developing models that can mirror such abilities in artificial systems.

I design dynamic dataset creation pipelines that include 3D scene construction grounded in physical principles and modular framework structures for flexible task generation. I believe that evaluating model capabilities should go beyond static benchmarks and limited multiple-choice evaluation formats, which fail to reflect a modelâ€™s true reasoning ability. Instead, evaluation must be open-ended, comprehensive, and human-aligned to uncover not only what a model gets wrong, but why and how that differs from human reasoning patterns.

In addition, I explore `parameter-efficient fine-tuning` and `preference-based reinforcement learning` as strategies to enhance the reasoning and problem-solving abilities of multimodal foundation models in complex, abstract domains.
